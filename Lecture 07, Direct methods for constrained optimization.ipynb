{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:95% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7, direct methods for constrained optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Structure of optimization methods\n",
    "\n",
    "Typically\n",
    "\n",
    "* Constraint handling **converts** the problem to (a series of) unconstrained problems\n",
    "* In unconstrained optimization a **search direction** is determined at each iteration\n",
    "* The best solution in the search direction is found with **line search**\n",
    "\n",
    "![](images/OptimizationStruc1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classification of the constraint optimization methods\n",
    "\n",
    "* **Indirect methods:** the constrained problem is converted into a sequence of unconstrained problems whose solutions will approach to the solution of the constrained problem, the intermediate solutions need not to be feasible \n",
    "\n",
    "* **Direct methods:** the constraints are taking into account explicitly, intermediate solutions are feasible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Direct methods for constrained optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Direct methods for constrained optimization are also known as *methods of feasible directions*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Idea\n",
    "\n",
    "* in a point $x_k$, generate a feasible search direction where objective function value can be improved\n",
    "* use line search to get $x_{k+1}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Methods differ in\n",
    "\n",
    "* how to choose a feasible direction and\n",
    "* what is assumed from the constraints (linear/nonlinear, equality/inequality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feasible descent directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let $S\\subset \\mathbb R^n$ ($S\\neq \\emptyset$) and $x^*\\in S$. \n",
    "\n",
    "**Definition:** The set\n",
    "$$ F = \\{d\\in \\mathbb R^n: d\\neq0,x^*+\\alpha d\\in S \\text{ for all } \\alpha\\in (0,\\delta) \\text{ for some } \\delta>0\\}$$\n",
    "\n",
    "is called the cone of feasible directions of $S$ in $x^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Definition:** The set \n",
    "$$ D = \\{d\\in \\mathbb R^n: f(x^*+\\alpha d)<f(x^*)\\text{ for all } \\alpha\\in (0,\\delta) \\text{ for some } \\delta>0\\}$$\n",
    "is called the cone of descent directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Definition:** The set $F\\cap D$ is called the cone of feasible descent directions.\n",
    "\n",
    "\n",
    "![alt text](images/feasible_descent_directions.svg \"Feasible descent directions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(Obvious) Theorem:** Consider an optimization problem \n",
    "$$\n",
    "\\begin{align}\n",
    "\\min &\\  f(x)\\\\\n",
    "\\text{s.t. }&\\ x\\in S\n",
    "\\end{align}\n",
    "$$\n",
    "and let $x^*\\in S$. Now if $x^*$ is a local minimizer **then** the set of feasible descent directions $F\\cap D$ is empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Idea for the methods of feasible descent directions\n",
    "\n",
    "1. Find a feasible solution $x_0$ as the starting point ($k=0$).\n",
    "2. Find a feasible descent direction $d_k\\in D\\cap F$.\n",
    "3. Determine the step length ($\\alpha_k$) to the direction $d_k$ (Use line search to find an optimal step length).\n",
    "4. Update $x$ accordingly ($x_{k+1} = x_k + \\alpha_k d_k$).\n",
    "5. Check convegency. If not converged, set $k = k+1$ and go to 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Rosen's projected gradient method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume a problem with linear equality constraints\n",
    "\n",
    "$$\n",
    "\\min f(x)\\\\\n",
    "\\text{s.t. } H(x)=Ax-b=0,\n",
    "$$\n",
    "\n",
    "where $A$ is an $l\\times n$ matrix ($l\\leq n$) and $b$ is a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let $\\mathbf{x}$ be a feasible solution to the above problem.\n",
    "\n",
    "It holds that:\n",
    "\n",
    "$$\n",
    "\\mathbf{Ax}=b \\\\\n",
    "\\rightarrow \\mathbf{A}(\\mathbf{x} + \\alpha \\mathbf{d}) = b \\\\\n",
    "\\rightarrow \\mathbf{Ax} + \\alpha \\mathbf{Ad} = b \\\\\n",
    "\\rightarrow b + \\alpha \\mathbf{Ad} = b\n",
    "$$\n",
    "\n",
    "Then, $\\mathbf{d}$ is a feasible direction *if and only if* $\\mathbf{Ad}=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus, the gradient $-\\nabla f(x)$ is a feasible descent direction, if \n",
    "\n",
    "$$ A\\nabla f(x)=0.$$\n",
    "\n",
    "This may or may not be true (i.e., the gradient may or may not be a feasible descent direction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, we can project the gradient to the set of feasible descent directions\n",
    "$$ \\{d\\in \\mathbb R^n: Ad=0\\},$$\n",
    "which now is a linear subspace.\n",
    "\n",
    "![alt text](images/subspace.svg \"A linear subspace Ad=0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Projection\n",
    "\n",
    "Let $a\\in \\mathbb R^n$ be a vector and let $L$ be a linear subspace of $\\mathbb R^n$. Now, the following are equivalent\n",
    "* $a^P$ is the projection of $a$ on $L$,\n",
    "* $\\{a^P\\} = \\operatorname{argmin}_{l\\in L}\\|a-l\\|$, \n",
    "(i.e.,  $a^P$, on $L$, has the minimum distance to $a$) and\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/projection.jpg )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Projected gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The projection of the gradient $\\nabla f(x)$ on the set $\\{d\\in \\mathbb R^n: Ad=0\\}$ is denoted by $\\nabla f(x)^P$ and called the *projected gradient*. \n",
    "\n",
    "Now, given some conditions, the projected gradient gives us a feasible descent direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/projected_gradient.svg \"A projected gradient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/RosenGrad.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to compute the projected gradient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways, but at this course we can use optimization. Basically, the optimization problem that we have to solve is\n",
    "$$\n",
    "\\min \\|\\nabla f(x)-d\\|\\\\\n",
    "\\text{s.t. }Ad=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since it is equivalent to minimize the square of the objective function $\\sum_{i=n}\\nabla_i f(x)^2+d_i^2-2\\nabla_i f(x)d_i$, we can see that the problem is a quadratic problem with equality constraints,\n",
    "$$\n",
    "\\min \\frac12 d^TId-\\nabla f(x)^Td\\\\\n",
    "\\text{s.t. }Ad=0\n",
    "$$\n",
    "which means that we just need to solve the system of equations (see e.g., https://en.wikipedia.org/wiki/Quadratic_programming#Equality_constraints)\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "I&A^T\\\\\n",
    "A&0\n",
    "\\end{array}\n",
    "\\right] \n",
    "\\left[\\begin{align}d\\\\\\lambda\\end{align}\\right]\n",
    "= \\left[ \n",
    "\\begin{array}{c}\n",
    "\\nabla f(x)\\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right],\n",
    "$$\n",
    "\n",
    "where $I$ is the identity matrix, and $\\lambda$ are the Lagrange multipliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Code in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A function for projecting a vector to a linear space defined by $Ax=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#help(np.linalg.solve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def project_vector(A,vector):\n",
    "    #convert A into a matrix\n",
    "    A_matrix = np.matrix(A)\n",
    "    #construct the \"first row\" of the matrix [[I,A^T],[A,0]]\n",
    "    left_matrix_first_row = np.concatenate((np.identity(len(vector)),A_matrix.transpose()), axis=1)\n",
    "    #construct the \"second row\" of the matrix\n",
    "    left_matrix_second_row = np.concatenate((A_matrix,np.matrix(np.zeros([len(A),len(A)]))), axis=1)\n",
    "    #combine the whole matrix by combining the rows\n",
    "    left_matrix = np.concatenate((left_matrix_first_row,left_matrix_second_row),axis = 0)\n",
    "    #Solve the system of linear equalities from the previous page\n",
    "    return np.linalg.solve(left_matrix, \\\n",
    "                           np.concatenate((np.matrix(vector).transpose(),\\\n",
    "                                           np.zeros([len(A),1])),axis=0))[:len(vector)] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Project gradient such that A*proj_gradient = 0\n",
    "A = [[1,0,0],[0,1,0]]\n",
    "gradient = [1,1,1]\n",
    "project_vector(A,gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us study optimization problem\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\qquad& x_1^2+x_2^2+x_3^2\\\\\n",
    "\\text{s.t.}\\qquad &x_1+x_2=3\\\\\n",
    "    &x_1+x_3=4.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let us project a negative gradient from a feasible point $x=(1,2,3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the matrix\n",
    "$$\n",
    "A = \\left[\n",
    "\\begin{array}{ccc}\n",
    "1& 1 & 0\\\\\n",
    "1& 0 & 1\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0, 4.0, 6.0]\n",
      "[[ 2.66666667]\n",
      " [-2.66666667]\n",
      " [-2.66666667]]\n"
     ]
    }
   ],
   "source": [
    "import ad\n",
    "A = [[1,1,0],[1,0,1]]\n",
    "gradient = ad.gh(lambda x:x[0]**2+x[1]**2+x[2]**2)[0]([1,2,3])\n",
    "print(gradient)\n",
    "d = project_vector(A,[-i for i in gradient])\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### d is a feasible direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matrix(A)*d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d is a descent direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of f at [1,2,3] is 14\n",
      "[2.33333333 0.66666667 1.66666667]\n",
      "Value of f at [1,2,3] +alpha*d is 8.666666666666668\n",
      "Gradient dot product direction (i.e., directional derivative) is [[-21.33333333]]\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x[0]**2+x[1]**2+x[2]**2\n",
    "alpha = 0.5\n",
    "print(\"Value of f at [1,2,3] is \"+str(f([1,2,3])))\n",
    "x_mod= np.array([1,2,3])+alpha*np.array(d).transpose()[0]\n",
    "print(x_mod)\n",
    "print(\"Value of f at [1,2,3] +alpha*d is \"+str(f(x_mod)))\n",
    "print(\"Gradient dot product direction (i.e., directional derivative) is \" \\\n",
    "      + str(np.matrix(ad.gh(f)[0]([1,2,3])).dot(np.array(d))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finally, the algorithm of the projected gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ad\n",
    "def projected_gradient_method(f,A,start,step,precision):\n",
    "    f_old = float('Inf')\n",
    "    x = np.array(start)\n",
    "    steps = []\n",
    "    f_new = f(x)\n",
    "    iters = 0\n",
    "    while abs(f_old-f_new)>precision:\n",
    "        # store the current function value\n",
    "        f_old = f_new\n",
    "        # compute gradient\n",
    "        gradient = ad.gh(f)[0](x)\n",
    "        # project negative gradient\n",
    "        d = project_vector(A,[-i for i in gradient])\n",
    "        # take transpose\n",
    "        d = d.reshape(1,-1)\n",
    "        # take step\n",
    "        x = np.array(x + step*d)[0]\n",
    "        # compute f in new point+ \n",
    "        f_new = f(x)\n",
    "        # record new step\n",
    "        steps.append(x)\n",
    "        # update iterations counter\n",
    "        iters = iters + 1\n",
    "    return x,f_new,steps,iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f = lambda x:x[0]**2+x[1]**2+x[2]**2\n",
    "A = [[1,1,0],[1,0,1]]\n",
    "start = [1,2,3]\n",
    "(x,f_val,steps,iters) = projected_gradient_method(f,A,start,0.5,0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.33333333 0.66666667 1.66666667]\n",
      "8.666666666666668\n",
      "14\n",
      "[[3.]\n",
      " [4.]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(f(x))\n",
    "print(f([1,2,3]))\n",
    "print(np.matrix(A)*np.matrix(x).transpose())\n",
    "print(iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Note\n",
    "If there are both linear equality and inequality constraints, the projection matrix does not remain the same \n",
    "* At each iteration, it includes only the equality and active inequality constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Active set method\n",
    "Consider a problem\n",
    "$$\n",
    "\\min f(x)\\\\\n",
    "\\text{s.t. }Ax\\leq b,\n",
    "$$\n",
    "where $A$ is an $l\\times n$ matrix ($l\\leq n$) and $b$ is a vector.\n",
    "\n",
    "## Idea\n",
    "* In $𝑥_k$, the set of constraints is divided into active ($𝑖 ∈ 𝐼$) and inactive constraints\n",
    "* Inactive constraints are not taken into account when the search direction $𝑑_k$ is determined\n",
    "* Inactive constraints affect only when computing the optimal step length $\\alpha_k$ (making sure we are not step out of the feasible region)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feasible directions\n",
    "* For $𝑖\\in 𝐼$ (active constraints), $(𝑎_i)^Tx_k = b_i$\n",
    "* If $𝑑_k$ is feasible in $𝑥_k$, then $𝑥_k + \\alpha 𝑑_k \\in 𝑆$ for some $\\alpha > 0$\n",
    "* $(𝑎_i)^T(x_k+\\alpha d_k) = (a_i)^Tx_k + \\alpha(a_i)^Td_k\\leq b_i$\n",
    "* $(𝑎_i)^Td_k\\leq 0$ for feasible $𝑑_k$ and the constraint remains active if $(𝑎_i)^Td_k=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## On active constraints\n",
    "* Optimization problem with inequality constraints is more difficult than problem with equality constraints since the active set in a local minimizer is not known\n",
    "* If it would be known, then it would be enough to solve a corresponding equality constrained problem\n",
    "* In that case, if the other constraints would be satisfied in the solution and all the Lagrange multipliers were non-negative, then the solution would also be a solution to the original problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Using active set\n",
    "* At each iteration, a working set is considered which consists of the active constraints in $𝑥_k$\n",
    "* The direction $𝑑_k$ is determined so that it is a descent direction in the working set\n",
    "  * E.g., Rosen’s projected gradient method can be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Active set algorithm\n",
    "1. Choose a starting point $x_1$ and determine an initial active set $I_1$ and set $k=1$\n",
    "2. Compute a feasible descent direction $d_k$ in the subspace defined by the active constraints (e.g., by using projected gradient)\n",
    "3. If $||d_k||=0$, go to step 6, otherwise, find optimal step length $\\alpha$ by staying in the feasible set and set $x_{k+1} = x_k + \\alpha d_k$\n",
    "4. If no new constraint becomes active go to step 7 (active set does not change)\n",
    "5. Addition to active set: a new constraint $j$ becomes active, update $I_{k+1} = I_k \\cup \\{j\\}$ and go to step 7\n",
    "6. Removal from active set: approximate Lagrangian multipliers $\\mu_i$, $i\\in I_k$. If $\\mu_i\\geq 0$ for all $i$, stop (active set is correct). Otherwise, remove a constraint $j$ with negative multiplier from the active set: $I_{k+1}=I_k\\setminus \\{j\\}$\n",
    "7. Set $k=k+1$ and go to step 2\n",
    "\n",
    "<i>Implementation of the active set method is left as a voluntary exercise </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Note: \n",
    "\n",
    "* Projected gradient method can also extended for non-linear constraints.\n",
    "* But, this needs some extra steps\n",
    "\n",
    "![](Images/project-nl.jpg)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
